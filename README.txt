â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Portable 2-D âœ 3-D Converter (2025 Depth-Anything Edition)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â–  OVERVIEW
   This tool converts any 2-D video into a half-side-by-side (HSBS)
   stereoscopic 3-D video, suitable for VR/AR playback. It uses:
     â€¢ Depth-Anything ViT-L-14 (state-of-the-art, metric depth, high detail)
     â€¢ CUDA-accelerated PyTorch for depth inference
     â€¢ OpenCV + FFmpeg (NVENC if available) for video I/O
     â€¢ Tkinter for a minimal GUI

   The entire appâ€”including model weights, FFmpeg, and Python packagesâ€”
   lives inside its folder. After the first run, you can move it anywhere.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–  SYSTEM REQUIREMENTS
   1.  Windows 10 or 11 (64-bit)
   2.  NVIDIA GPU with CUDA 11.8+ (recent driver installed)
   3.  Python 3.9 â€“ 3.12 on PATH (only for first-time run)
       â€¢ used to create the virtual environment
       â€¢ after that, the included env is used
   4.  ~3 GB free disk space in this folder (for venv, model, ffmpeg, temp)
   5.  Internet connection on **first run** (to allow MiDaS to be cloned)
       â€¢ MiDaS is pulled via torch.hub at runtime; afterward itâ€™s cached.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–  FIRST-TIME SET-UP (one click)
   1. Place these files side by side in a new folder:
        â€¢ run_converter.bat
        â€¢ converter.py
        â€¢ README.txt
   2. Double-click **run_converter.bat**.
      The script will:
        â–¸ Create `env\` (virtual environment)
        â–¸ pip-install:
            â€¢ torch, torchvision, torchaudio (CUDA 11.8)
            â€¢ opencv-python, numpy, tqdm, timm
        â–¸ Download FFmpeg static build (~70 MB) â†’ `ffmpeg.exe`
        â–¸ Download Depth-Anything ViT-L-14 weights (~450 MB) â†’ `models\depth_anything_vitl14.pth`
        â–¸ Launch `converter.py` using the new `env\`
      All of that may take **3â€“6 minutes** (depends on your Internet speed and GPU driver checks).

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–  HOW TO USE
   1. After setup, a small console window and Tkinter dialog will appear.
   2. Choose your source 2-D video (any common format: .mp4, .mkv, .mov, .avi).
   3. Select an output folder (where the 3-D video will be saved).
   4. Enter â€œMax pixel shiftâ€ (1â€“100). This controls perceived depth:
      â€¢ ~15â€“40 px is usually comfortable on most VR headsets.
   5. The console shows a progress bar (â€œConverting (NVENC)â€ or â€œConverting (CPU)â€).
      â€¢ GPU (NVENC) encoding happens if your system supports it; otherwise falls back to CPU.
   6. When done, youâ€™ll see a popup with:
        Saved stereo video:
          <output_folder>\YourVideo_3D_HSBS.mp4
        Elapsed: X.Y min
      This final file:
        â€¢ Contains side-by-side (left|right) 3-D frames
        â€¢ Has the original audio (losslessly copied via FFmpeg)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–  FILES & FOLDERS AFTER SETUP
   â€¢ converter.py              â† main Python script
   â€¢ run_converter.bat         â† launcher/installer
   â€¢ README.txt                â† this guide
   â€¢ env\                      â† Python virtual environment (auto-created)
       (inside env, youâ€™ll see standard venv folders: Scripts\, Lib\, etc.)
   â€¢ models\                   â† folder created by BAT
       â€¢ depth_anything_vitl14.pth
   â€¢ ffmpeg.exe                â† downloaded static FFmpeg build
   â€¢ run_converter.log         â† log file (captures setup + runtime messages)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–  TROUBLESHOOTING

  1. â€œCUDA not availableâ€ or â€œTorch cannot find a GPUâ€  
     â€¢ Ensure you have an NVIDIA GPU and have installed the latest Game/Studio driver.  
     â€¢ If you only have CPU, the script will still work (just slower).

  2. â€œCould not open VideoWriter with any codecâ€  
     â€¢ Means neither â€œH264 (NVENC)â€ nor â€œmp4v (CPU)â€ was available.  
     â€¢ Solution: install the *K-Lite Codec Pack* (free) or check your OpenCV build.

  3. â€œNo module named 'cv2'â€ or similar after BAT finishes  
     â€¢ Make sure the BAT did not error in the â€œpip installâ€ step.  
     â€¢ Re-run `run_converter.bat` in a console so you can see any pip errors.

  4. â€œError: Model weights not foundâ€  
     â€¢ Ensure â€œmodels\depth_anything_vitl14.pthâ€ exists.  
     â€¢ If the BATâ€™s download failed, fetch manually from:  
       https://huggingface.co/isl-org/DepthAnything/resolve/main/depth_anything_vitl14.pth  
     â€¢ Place the file at exactly `models\depth_anything_vitl14.pth`.

  5. â€œMiDaS clone failed (no internet?)â€  
     â€¢ On first launch, converter.py will call `torch.hub.load("intel-isl/MiDaS", â€¦)`.  
       This needs internet to clone MiDaS. After that, itâ€™s cached.  
     â€¢ If you absolutely cannot connect to the internet, youâ€™d need to pre-clone MiDaS:
         a) Open a separate PowerShell / CMD.  
         b) Run:  
            git clone https://github.com/isl-org/MiDaS.git %USERPROFILE%\.cache\torch\hub\intel-isl_MiDaS  
         c) Then re-run the converter.  
       (Torch will detect the local MiDaS cache and skip downloading.)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–  PERFORMANCE & TIPS

  â€¢ Depth-Anything is a DPT_Large backbone â†’ each 720p frame may take ~1â€“2 seconds to infer on a
    modern NVIDIA GPU (e.g., RTX 20xx, 30xx, or 40xx). Higher resolutions take longer.

  â€¢ If you want faster â€œpreviewâ€ (lower quality), reduce the video resolution or use a smaller
    â€œMax pixel shift.â€ Also, you could swap the depth model in `converter.py` to MiDaS directly
    by replacing `load_depth_anything()` with plain MiDaS (still via torch.hub).

  â€¢ â€œMax pixel shiftâ€ controls 3-D strength. Too large â†’ uncomfortable or â€œexaggeratedâ€ depth.
    Too small â†’ 3-D effect looks flat. Start at 20â€“30 px, then adjust.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Enjoy crisp, artifact-free 3-D conversions! ğŸ‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
